{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x201ee961670>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision\n",
    "import time\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used:  cuda\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(\"Device being used: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mean= 0.13062754273414612 \n",
    "scaled_std= 0.30810779333114624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  torchvision.datasets.MNIST('./data/files/', train=True, download=True,  transform=torchvision.transforms.Compose([\n",
    "                                                                                       torchvision.transforms.ToTensor(),\n",
    "                                                                                       torchvision.transforms.Normalize((scaled_mean,), (scaled_std,))]))\n",
    "test_dataset =   torchvision.datasets.MNIST('./data/files/', train=False, download=True,  transform=torchvision.transforms.Compose([\n",
    "                                                                                       torchvision.transforms.ToTensor(),\n",
    "                                                                                       torchvision.transforms.Normalize((scaled_mean,), (scaled_std,))]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=use_gpu)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True, pin_memory=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set: 60000, size of the test set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the train set: {len(train_loader.dataset)}, size of the test set: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_dataset[0][0].numpy().squeeze(), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 256) \n",
    "        self.linear2 = nn.Linear(256, 100) \n",
    "        self.final = nn.Linear(100, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img): #convert + flatten\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.final(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    Reset model weights to avoid weight leakage. Useful during hyperparameter tuning\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    '''\n",
    "    Train the given model, using te specified train data (dataloader), loss (criterion) and optimizer\n",
    "    '''\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        \n",
    "        #Transfer batches to GPU\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def test(model, dataloader):\n",
    "    '''\n",
    "    Evaluate the accuracy of the given model, tested on the specified test set (dataloader)\n",
    "    '''\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        \n",
    "        #Transfer batches to GPU\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(x_batch)           # output of linear\n",
    "            probs = F.softmax(preds, dim=1) # probability distribution\n",
    "            preds = probs.argmax(dim=1)      # most probable class (for each sample in the batch)\n",
    "\n",
    "            correct += (preds == y_batch).sum()\n",
    "            total += len(preds)\n",
    "            \n",
    "    return correct / total # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick test to see if things are working and to learn how LR-schedulers work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: test accuracy 0.9017\n",
      "Epoch 1: test accuracy 0.9236\n",
      "Epoch 2: test accuracy 0.9344\n",
      "Epoch 3: test accuracy 0.9414\n",
      "Epoch 4: test accuracy 0.9506\n",
      "Epoch 5: test accuracy 0.9536\n",
      "Epoch 6: test accuracy 0.9596\n",
      "Epoch 7: test accuracy 0.9626\n",
      "Epoch 8: test accuracy 0.9644\n",
      "Epoch 9: test accuracy 0.9677\n",
      "Epoch 10: test accuracy 0.9687\n",
      "Epoch 11: test accuracy 0.9692\n",
      "Epoch 12: test accuracy 0.969\n",
      "Epoch 13: test accuracy 0.9721\n",
      "Epoch 14: test accuracy 0.9728\n",
      "Epoch 15: test accuracy 0.9734\n",
      "Epoch 16: test accuracy 0.9742\n",
      "Epoch 17: test accuracy 0.9754\n",
      "Epoch 18: test accuracy 0.9753\n",
      "Epoch 19: test accuracy 0.9753\n",
      "--- 397.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(model, train_loader, torch.nn.CrossEntropyLoss(), optimizer)\n",
    "    acc = test(model, test_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch}: test accuracy {acc:.4}')\n",
    "    \n",
    "print(f\"--- {((time.time() - start_time)/60):.4} minutes ---\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: test accuracy 0.9025\n",
      "Epoch 1: test accuracy 0.9257\n",
      "Epoch 2: test accuracy 0.9367\n",
      "Epoch 3: test accuracy 0.944\n",
      "Epoch 4: test accuracy 0.9506\n",
      "Epoch 5: test accuracy 0.9556\n",
      "Epoch 6: test accuracy 0.9628\n",
      "Epoch 7: test accuracy 0.9665\n",
      "Epoch 8: test accuracy 0.9717\n",
      "Epoch 9: test accuracy 0.9716\n",
      "Epoch 10: test accuracy 0.9717\n",
      "Epoch 11: test accuracy 0.9752\n",
      "Epoch 12: test accuracy 0.9772\n",
      "Epoch 13: test accuracy 0.9721\n",
      "Epoch 14: test accuracy 0.9789\n",
      "Epoch 15: test accuracy 0.9779\n",
      "Epoch 16: test accuracy 0.9674\n",
      "Epoch 17: test accuracy 0.9712\n",
      "Epoch 18: test accuracy 0.9788\n",
      "Epoch 19: test accuracy 0.9797\n",
      "--- 381.2 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(model, train_loader, torch.nn.CrossEntropyLoss(), optimizer)\n",
    "    acc = test(model, test_loader)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}: test accuracy {acc:.4}')\n",
    "    \n",
    "print(f\"--- {((time.time() - start_time)/60):.4} minutes ---\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: test accuracy 0.9596\n",
      "Epoch 1: test accuracy 0.9746\n",
      "Epoch 2: test accuracy 0.9768\n",
      "Epoch 3: test accuracy 0.9751\n",
      "Epoch 4: test accuracy 0.9792\n",
      "Epoch 5: test accuracy 0.9819\n",
      "Epoch 6: test accuracy 0.9821\n",
      "Epoch 7: test accuracy 0.9828\n",
      "Epoch 8: test accuracy 0.9818\n",
      "Epoch 9: test accuracy 0.9837\n",
      "Epoch 10: test accuracy 0.9831\n",
      "Epoch 11: test accuracy 0.9832\n",
      "Epoch 12: test accuracy 0.9827\n",
      "Epoch 13: test accuracy 0.983\n",
      "Epoch 14: test accuracy 0.9835\n",
      "Epoch 15: test accuracy 0.9831\n",
      "Epoch 16: test accuracy 0.983\n",
      "Epoch 17: test accuracy 0.9829\n",
      "Epoch 18: test accuracy 0.9833\n",
      "Epoch 19: test accuracy 0.9834\n",
      "--- 408.5 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=1/2)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(model, train_loader, torch.nn.CrossEntropyLoss(), optimizer)\n",
    "    acc = test(model, test_loader)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}: test accuracy {acc:.4}')\n",
    "    \n",
    "print(f\"--- {((time.time() - start_time)/60):.4} minutes ---\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1 lr: 0.01\n",
      "Epoch-2 lr: 0.01\n",
      "Epoch-3 lr: 0.01\n",
      "Epoch-4 lr: 0.01\n",
      "Epoch-5 lr: 0.02\n",
      "Epoch-6 lr: 0.02\n",
      "Epoch-7 lr: 0.02\n",
      "Epoch-8 lr: 0.02\n",
      "Epoch-9 lr: 0.02\n",
      "Epoch-10 lr: 0.04\n",
      "Epoch-11 lr: 0.04\n",
      "Epoch-12 lr: 0.04\n",
      "Epoch-13 lr: 0.04\n",
      "Epoch-14 lr: 0.04\n",
      "Epoch-15 lr: 0.08\n",
      "Epoch-16 lr: 0.08\n",
      "Epoch-17 lr: 0.08\n",
      "Epoch-18 lr: 0.08\n",
      "Epoch-19 lr: 0.08\n",
      "Epoch-20 lr: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Just to check how the scheduler will work\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=2)\n",
    "for epoch in range(1, 21):\n",
    "    scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k_fold Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cross_validation(model, parameters):\n",
    "    '''\n",
    "    Perform a K-fold training with CV using the specified parameters.\n",
    "    This is done by splitting the train set into K-different folds, and performing a training and testing step \n",
    "    using the compinations of the folds as new training and validation sets.\n",
    "    \n",
    "    This whole procedure is done using the specified parameters for number of folds, loss function, number of epochs, batch size, and optimizer.\n",
    "    '''\n",
    "    \n",
    "    #Define the Kfold to be used when splitting the training dataset\n",
    "    kfold = KFold(n_splits=parameters[\"folds\"], shuffle = True)\n",
    "    results = {}\n",
    "    \n",
    "    #Itereate K number of times, each time changing the indeces of the train/validation sets\n",
    "    for fold, (train_index, validation_index) in enumerate(kfold.split(train_dataset)):\n",
    "        \n",
    "        print(\" -Fold Nr: \", fold+1)\n",
    "        \n",
    "        # Create the Training and Validation sets, using the assigned indeces\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
    "        validation_subsampler = torch.utils.data.SubsetRandomSampler(validation_index)\n",
    "        \n",
    "        #Use the previous datasets to create DataLoaders with the specified batch_size\n",
    "        train_loader_Kfold = torch.utils.data.DataLoader(train_dataset, sampler=train_subsampler, batch_size=parameters[\"batch_size\"], pin_memory=use_gpu)\n",
    "        validation_loader_Kfold  = torch.utils.data.DataLoader(train_dataset, sampler=validation_subsampler, batch_size=parameters[\"batch_size\"], pin_memory=use_gpu)        \n",
    "        \n",
    "\n",
    "        #Reset the model weights for the new iteration\n",
    "        model.apply(reset_weights)\n",
    "        \n",
    "        #Define the criterion, optimizer and scheduler as specified in the parameters\n",
    "        criterion = parameters[\"criterion\"]()\n",
    "\n",
    "        optimizer = parameters[\"optimizer\"](model.parameters(), lr=parameters[\"lr\"])   \n",
    "        \n",
    "        scheduler = parameters[\"scheduler\"](optimizer, **parameters[\"scheduler_parameters\"])\n",
    "       \n",
    "\n",
    "        #Repeat the training step 'epoch'-number of times. Keep track of the accuracy when testing on the current validation set\n",
    "        for epoch in range(parameters[\"epochs\"]):\n",
    "            \n",
    "            train(model, train_loader_Kfold, criterion, optimizer)\n",
    "            \n",
    "            results[fold] = 100.0 * (test(model, validation_loader_Kfold))\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f\"      Epoch: {epoch+1}/{parameters['epochs']}, Validation Score: {results[fold].item():.4}\")\n",
    "\n",
    "    # Print fold results\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    avg_acc = sum/len(results.items())\n",
    "    print(f'   Average validation score of the {parameters[\"folds\"]} folds : {avg_acc:.4} %')\n",
    "    print('--------------------------------')\n",
    "        \n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, parameters):\n",
    "    '''\n",
    "    Perform a grid search on the model using the given parameters, calling K-fold cross validation for each combination. \n",
    "    After all the combinations of parameters have been tried, return the parameters that had the best average validation accuracy.\n",
    "    '''\n",
    "    keys, values = zip(*parameters.items())\n",
    "    permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_settings = {}\n",
    "    \n",
    "    for params in tqdm(permutations_dicts):\n",
    "        print(\"-CURRENT PARAMETERS : \", params)\n",
    "        acc = train_with_cross_validation(model, params)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_settings = params\n",
    "    print(f\"Best overall accuracy {best_acc} for these parameters {best_settings}\")\n",
    "    return best_settings, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_SGD_StepLR = {\n",
    "    \"criterion\": [torch.nn.CrossEntropyLoss],\n",
    "    \"optimizer\": [torch.optim.SGD],\n",
    "    \"scheduler\": [StepLR],\n",
    "    \"scheduler_parameters\": [{'step_size':5, 'gamma':1/2}, {'step_size':5, 'gamma':2} ],\n",
    "    \"lr\": [0.1],\n",
    "    \"batch_size\": [50, 100],\n",
    "    \"folds\": [3],\n",
    "    \"epochs\": [20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "best_parameters, best_accuracy = grid_search(model, parameters_SGD_StepLR)\n",
    "\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
